{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "The following examples demonstrate how to use the ``high5py`` module to interact with HDF5 files quickly and easily.\n",
    "\n",
    "## Setup\n",
    "\n",
    "To set up for the tutorial, we first we import all the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import high5py as hi5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we remove any test files that may have been generated by a previous run of the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filepath in glob.glob('test*.h5'):\n",
    "    os.remove(filepath)\n",
    "for filepath in glob.glob('test*.npz'):\n",
    "    os.remove(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create some data that we will later save to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random((10, 20))\n",
    "y = 2. * x.T\n",
    "z = x ** 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data\n",
    "\n",
    "We can save data using the `save_dataset` function, which **overwrites the file by default** (to avoid this, see the section below on appending data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi5.save_dataset('test.h5', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the syntax above, the data is saved to the file \"test.h5\" with the default dataset name \"data.\"\n",
    "We can check this using the `info` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi5.info('test.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that as expected, the file ``test.h5`` contains a single dataset called \"data.\"\n",
    "To save the dataset with a custom name, we use the ``name`` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi5.save_dataset('test.h5', x, name='x')\n",
    "hi5.info('test.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset is called \"x.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending data (safe saving)\n",
    "\n",
    "By default, `save_dataset` overwrites files.\n",
    "To add a dataset to the file, we use `append_dataset`, which is equivalent to calling `save_dataset` with ``overwrite=False``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi5.append_dataset('test.h5', y, name='y')\n",
    "hi5.info('test.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can use `append_dataset` to save a new file as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi5.append_dataset('test2.h5', y, name='y')\n",
    "hi5.info('test2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we attempt to append a dataset \"y\" to a file that already contains a dataset with that name, we will get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    hi5.append_dataset('test2.h5', y, name='y')\n",
    "except RuntimeError as err:\n",
    "    print('RuntimeError: {}'.format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, users who wish to avoid overwriting files and/or datasets can use `append_dataset` as a safer alternative to `save_dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing data\n",
    "\n",
    "It is sometimes desirable to overwrite a single dataset without overwriting the entire file. \n",
    "This can be done using `replace_dataset`, which deletes the existing dataset and replaces it with the specified one.\n",
    "Here we will replace the dataset \"x\" with a scalar value of 0.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi5.replace_dataset('test.h5', 0., name='x')\n",
    "hi5.info('test.h5', name='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will replace it with its original values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi5.replace_dataset('test.h5', x, name='x')\n",
    "hi5.info('test.h5', name='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data with descriptions\n",
    "\n",
    "Since one of the advantages of HDF5 is that it is a self-describing file format, ``high5py`` provides an easy way to add descriptions when saving datasets.\n",
    "To do so, simply use the ``description`` parameter (available for both `save_dataset` and `append_dataset`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi5.save_dataset('test.h5', x, name='x', description='x data')\n",
    "hi5.append_dataset('test.h5', x, name='y', description='y data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the value of the dataset descriptions by using the `info` function with the appropriate ``name`` value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi5.info('test.h5', name='x')\n",
    "print()\n",
    "hi5.info('test.h5', name='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data in groups\n",
    "\n",
    "We can also save data in groups by using the ``name`` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi5.append_dataset('test.h5', x, name='xy_group/x')\n",
    "hi5.append_dataset('test.h5', y, name='xy_group/y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that ``test.h5`` contains two datasets (\"x\" and \"y\") and a group (\"xy_group\") at the root level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi5.info('test.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get info on the contents of the group using the ``info`` function with the ``name`` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi5.info('test.h5', name='xy_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Loading data is simple using `hi5.load_dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_load = hi5.load_dataset('test.h5', name='x')\n",
    "print('Max diff b/w orig and loaded x: {:.2e}'.format(np.abs(x - x_load).max()))\n",
    "y_load = hi5.load_dataset('test.h5', name='xy_group/y')\n",
    "print('Max diff b/w orig and loaded y: {:.2e}'.format(np.abs(y - y_load).max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the ``name`` parameter defaults to \"data,\" so that `save_dataset` and `load_dataset` have compatible defaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi5.save_dataset('test_defaults.h5', x)\n",
    "x_load = hi5.load_dataset('test_defaults.h5')\n",
    "print('Max diff b/w orig and loaded x: {:.2e}'.format(np.abs(x - x_load).max()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying files\n",
    "\n",
    "Sometimes it is useful to query a dataset and look at its contents.\n",
    "As we have seen above, we can use `info` to get info on groups and datasets.  If we set ``return_info=True``, then we can also return a dictionary of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('FILE/ROOT INFO:')\n",
    "hi5.info('test.h5')\n",
    "print('\\nGROUP INFO:')\n",
    "hi5.info('test.h5', name='xy_group')\n",
    "print('\\nDATASET INFO:')\n",
    "info = hi5.info('test.h5', name='xy_group/x', return_info=True)\n",
    "print('\\nDATASET INFO DICT:', info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check for the existence of a particular dataset or group using `exists`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset x exists:', hi5.exists('test.h5', 'x'))\n",
    "print('Dataset z exists:', hi5.exists('test.h5', 'z'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use `list_all` to recursively list the contents of a file or group, using the ``return_info`` parameter to return a dictionary of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('FILE/ROOT INFO:')\n",
    "info = hi5.list_all('test.h5')\n",
    "print('\\nGROUP INFO:')\n",
    "info = hi5.list_all('test.h5', name='xy_group', return_info=True)\n",
    "print('\\nGROUP INFO DICT:')\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving attributes\n",
    "\n",
    "As alluded to above, part of what makes HDF5 a self-describing file format is that groups and datasets can have associated attributes.\n",
    "We can use `save_attributes` or `append_attributes` to add attributes to a group or dataset, with the former overwriting any existing attributes and the latter simply adding to them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi5.save_dataset('test.h5', 'x', name='x')\n",
    "print('DATA W/O ATTRIBUTES')\n",
    "hi5.info('test.h5', 'x')\n",
    "hi5.save_attributes('test.h5', {'units': 'm/s', 'num_pts': x.size}, name='x')\n",
    "print('\\nDATA W/ATTRIBUTES')\n",
    "hi5.info('test.h5', 'x')\n",
    "hi5.append_attributes('test.h5', {'color': 'red'}, name='x')\n",
    "print('\\nDATA W/ADDED ATTRIBUTES')\n",
    "hi5.info('test.h5', 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming or deleting objects\n",
    "\n",
    "We can easily rename a dataset or group using `rename`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nORIGINAL DATA')\n",
    "hi5.info('test.h5')\n",
    "hi5.info('test.h5', 'x')\n",
    "print('\\nRENAMED DATA')\n",
    "hi5.rename('test.h5', 'x', 'x_new')\n",
    "hi5.info('test.h5')\n",
    "hi5.info('test.h5', 'x_new')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can delete a dataset or group using `delete`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nDELETED DATA')\n",
    "hi5.delete('test.h5', 'x_new')\n",
    "hi5.info('test.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with NPZ files\n",
    "\n",
    "Sometimes when collaborating, it is useful to have code with as few dependencies as possible.\n",
    "To help with that, ``high5py`` offers methods for converting HDF5 files to and from NPZ (numpy archive) format.\n",
    "For instance, the following code saves data to HDF5, then converts the entire contents of that file to NPZ using `to_npz`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi5.save_dataset('test.h5', x, name='xy_group/x')\n",
    "hi5.append_dataset('test.h5', y, name='xy_group/y')\n",
    "hi5.append_dataset('test.h5', z, name='z1')\n",
    "hi5.append_dataset('test.h5', 2. * z, name='z2')\n",
    "hi5.to_npz('test.h5', 'test_all.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save single groups/datasets, or lists of groups/datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi5.to_npz('test.h5', 'test_z1.npz', name='z1')\n",
    "hi5.to_npz('test.h5', 'test_z.npz', name=['z1', 'z2'])\n",
    "hi5.to_npz('test.h5', 'test_xy_group.npz', name='xy_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load data in an NPZ file, we can use the following syntax, noting that since NPZ files don't support groups, group/dataset paths have been altered by replacing slashes with underscores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('test_all.npz', 'r') as data:\n",
    "    print('NPZ contents:', data._files)\n",
    "    x = data['xy_group_x']\n",
    "    y = data['xy_group_y']\n",
    "    z1 = data['z1']\n",
    "    z2 = data['z2']\n",
    "with np.load('test_z1.npz', 'r') as data:\n",
    "    print('NPZ contents:', data._files)\n",
    "    z1 = data['z1']\n",
    "with np.load('test_z.npz', 'r') as data:\n",
    "    print('NPZ contents:', data._files)\n",
    "    z1 = data['z1']\n",
    "    z2 = data['z2']\n",
    "with np.load('test_xy_group.npz', 'r') as data:\n",
    "    print('NPZ contents:', data._files)\n",
    "    x = data['x']\n",
    "    y = data['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When converting an NPZ file to HDF5, array names are preserved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('test.npz', x_npz=x, y_npz=y)\n",
    "hi5.from_npz('test.npz', 'test.h5')\n",
    "hi5.info('test.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "We finish by removing any generated test files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filepath in glob.glob('test*.h5'):\n",
    "    os.remove(filepath)\n",
    "for filepath in glob.glob('test*.npz'):\n",
    "    os.remove(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
